Predicting Residential Electricity Consumption Using CNN-BiLSTM-Self-Attention
ğŸ“Œ Overview

Accurate electricity consumption forecasting is essential for efficient energy management, demandâ€“supply balancing, and smart grid planning. Traditional statistical methods struggle to capture the complex temporal patterns and non-linear dependencies present in real-world electricity consumption data.

This project proposes a deep learning-based framework for residential electricity load prediction using Convolutional Neural Networks (CNN), Bidirectional Long Short-Term Memory (BiLSTM), and a Self-Attention mechanism. The model effectively learns both local features and long-term temporal dependencies, while selectively focusing on the most relevant time steps.

ğŸ¯ Objectives

Predict short-term residential electricity consumption accurately

Capture temporal dependencies in time-series electricity data

Reduce noise and improve generalization using attention mechanisms

Compare a baseline CNN-LSTM Autoencoder with an advanced CNN-BiLSTM-Self-Attention model

ğŸ§  Models Implemented
1ï¸âƒ£ Baseline Model: CNN-LSTM Autoencoder

The initial model uses:

CNN layers for feature extraction

LSTM encoderâ€“decoder (autoencoder) for temporal sequence reconstruction

This model learns compressed representations of historical electricity consumption and reconstructs future values.

Limitations:

Sensitive to local fluctuations

Can overreact to noise

Limited ability to model long-range bidirectional dependencies

2ï¸âƒ£ Proposed Model: CNN-BiLSTM-Self-Attention (Final Model)

The enhanced model replaces the autoencoder with:

Bidirectional LSTM (BiLSTM) to learn past and future context simultaneously

Self-Attention mechanism to emphasize important time steps and suppress noise

Architecture:
Input Sequence
 â†’ CNN (feature extraction)
 â†’ CNN
 â†’ MaxPooling
 â†’ BiLSTM (bidirectional temporal learning)
 â†’ Self-Attention (important time-step weighting)
 â†’ Dense layer (prediction)


Advantages:

Better temporal understanding

Improved generalization

More stable predictions

Reduced sensitivity to noisy spikes

ğŸ“‚ Project Structure
â”œâ”€â”€ dataset/
â”‚   â””â”€â”€ IHEPC.csv
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ model.h5                     # CNN-LSTM Autoencoder model
â”‚   â””â”€â”€ model_CNN_BiLSTM_SA.h5       # CNN-BiLSTM-SA trained model
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ actual_predicted.png
â”‚   â””â”€â”€ actual_predicted_CNN_BiLSTM_SA.png
â”œâ”€â”€ Training.py                      # Baseline CNN-LSTM Autoencoder
â”œâ”€â”€ Training_CNN_BiLSTM_SA.py        # Proposed CNN-BiLSTM-SA model
â”œâ”€â”€ Testing.py
â””â”€â”€ README.md


ğŸ“Š Dataset

Source: Residential electricity consumption dataset

Features used:

datetime

Global_active_power

Missing values are handled using mean imputation

Data is normalized using Min-Max scaling

ğŸ”„ Data Preprocessing

Load dataset and parse timestamps

Handle missing values

Normalize electricity consumption values

Convert time series into sliding windows

Input window size: 8 time steps

Output prediction: next 4 time steps

Split data into training and testing sets

âš™ï¸ Model Training

Optimizer: Adam

Loss function: Mean Squared Error (MSE)

Epochs: 10

Batch size: 32

Validation split: 20%

ğŸ“ˆ Evaluation Metrics

The models are evaluated using:

MSE (Mean Squared Error)

RMSE (Root Mean Squared Error)

MAE (Mean Absolute Error)

MAPE (Mean Absolute Percentage Error)

Note: MAPE is sensitive to near-zero electricity values and may not always reflect true predictive performance.

ğŸ“Š Results & Comparison
ğŸ”¹ CNN-LSTM Autoencoder (Baseline)

Produces sharper predictions

More sensitive to short-term fluctuations

Slightly noisier output

ğŸ”¹ CNN-BiLSTM-Self-Attention (Proposed)

Produces smoother and more stable predictions

Lower RMSE and MSE

Better generalization

Attention mechanism suppresses irrelevant noise

Key Insight:
Although peak amplitudes are slightly reduced, the CNN-BiLSTM-Self-Attention model demonstrates improved robustness and lower prediction error.

ğŸ–¼ï¸ Output Visualization

The results folder contains plots comparing actual vs predicted electricity consumption for both models:

actual_predicted.png (baseline)

actual_predicted_CNN_BiLSTM_SA.png (proposed)

ğŸ§ª How to Run the Project
â–¶ï¸ Train Baseline Model
python Training.py

â–¶ï¸ Train Proposed CNN-BiLSTM-SA Model
python Training_CNN_BiLSTM_SA.py

ğŸš€ Future Work

Incorporate weather and occupancy features

Extend prediction horizon

Apply transformer-based architectures

Optimize attention mechanisms for peak load prediction

ğŸ“ Conclusion

This project demonstrates that integrating Bidirectional LSTM and Self-Attention significantly enhances electricity load forecasting performance compared to a standard CNN-LSTM autoencoder. The proposed CNN-BiLSTM-Self-Attention framework effectively captures complex temporal dependencies and offers a robust solution for real-world residential energy forecasting.

ğŸ‘¤ Author

Annavarapu Rohith
ğŸ“§ rohithannavarapu7@gmail.com
